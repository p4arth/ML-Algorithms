{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLT_WEEK_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wsj3xy_qIzOs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseOptimizers():\n",
        "  def __init__(self, optimizer = 'GD'):\n",
        "    self.optimizer = optimizer\n",
        "  \n",
        "  def loss(self, X, w, y):\n",
        "    assert X.shape[-1] == w.shape[0], 'Incompatible shapes'\n",
        "    y_hat = X @ w\n",
        "    loss_ = np.sum(np.square(y_hat - y))\n",
        "    return loss_\n",
        "\n",
        "  def lr_schedule(self,t):\n",
        "    t0, t1 = 200, 100000\n",
        "    return t0 / (t + t1)\n",
        "  \n",
        "  def gradient(self, X, w, y):\n",
        "    assert X.shape[-1] == w.shape[0], 'Incompatible shapes'\n",
        "    return X.T @ ((X @ w) - y)\n",
        "  \n",
        "  def gradient_descent(self, X, y, \n",
        "                       verbose, epochs, lr):\n",
        "    w0 = np.random.normal(0, 1, size=(X.shape[1],1))\n",
        "    self.all_weights = []\n",
        "    for epoch in range(epochs):\n",
        "      if verbose:\n",
        "        print('The Current Loss is :', self.loss(X, w0, y))\n",
        "      self.all_weights.append(w0)\n",
        "      w0 = w0 - lr*(self.gradient(X, w0, y))\n",
        "    return w0\n",
        "\n",
        "  def mini_batch_gd(self, X, y,\n",
        "                    verbose, epochs, batch_size):\n",
        "    w0 = np.random.normal(0, 1, size=(X.shape[1],1))\n",
        "    t = 0\n",
        "    self.all_weights = []\n",
        "    for epoch in range(epochs):\n",
        "      random_indices = np.random.permutation(X.shape[0])\n",
        "      X_shuffled = X[random_indices]\n",
        "      y_shuffled = y[random_indices]\n",
        "      for i in range(0, X.shape[0], batch_size):\n",
        "        t = t + 1\n",
        "        X_temp = X_shuffled[i:i+batch_size]\n",
        "        y_temp = y_shuffled[i:i+batch_size]\n",
        "        lr = self.lr_schedule(t)\n",
        "        self.all_weights.append(w0)\n",
        "        w0 = w0 - lr*(self.gradient(X_temp, w0, y_temp))\n",
        "      if verbose:\n",
        "          print(f'Epoch {epoch} Loss is :', self.loss(X, w0, y))\n",
        "    return w0\n",
        "\n",
        "  def stochastic_gd(self, X,  \n",
        "                    y, verbose, epochs):\n",
        "    w0 = np.random.normal(0, 1, size=(X.shape[1],1))\n",
        "    t = 0\n",
        "    self.all_weights = []\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(X.shape[0]):\n",
        "        random_index = np.random.randint(X.shape[0])\n",
        "        t = t + 1\n",
        "        X_temp = X[random_index:random_index+1]\n",
        "        y_temp = y[random_index:random_index+1]\n",
        "        lr = self.lr_schedule(t)\n",
        "        self.all_weights.append(w0)\n",
        "        w0 = w0 - lr*(self.gradient(X_temp, w0, y_temp))\n",
        "      if verbose:\n",
        "          print(f'Epoch {epoch} Loss is :', self.loss(X, w0, y))\n",
        "    return w0"
      ],
      "metadata": {
        "id": "5QhMWDo35ynU"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "5Yof_4VeHwjF"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(BaseOptimizers):\n",
        "  def __init__(self,\n",
        "               optimizer = 'GD',\n",
        "               random_seed = 42):\n",
        "    super().__init__(optimizer)\n",
        "    # self.optimizer = optimizer\n",
        "    self.random_seed = random_seed\n",
        "  \n",
        "  def add_dummy_feature(self, X):\n",
        "    matrix_dummy = np.hstack((np.ones((X.shape[0], 1),\n",
        "                                            dtype = X.dtype), \n",
        "                                            X))\n",
        "    return matrix_dummy\n",
        "  \n",
        "  def preprocess(self, X):\n",
        "    X = self.add_dummy_feature(X)\n",
        "    return X\n",
        "  \n",
        "  def train(self,\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs = 200,\n",
        "            batch_size = 100,\n",
        "            learning_rate = 0.001,\n",
        "            verbose = False):\n",
        "    assert batch_size < X_train.shape[0], 'batch size must be smaller than the number of data points'\n",
        "    X_train = self.preprocess(X_train)\n",
        "    if self.optimizer is 'GD':\n",
        "      self.optimized_weights = self.gradient_descent(X_train, y_train,\n",
        "                                                    verbose, epochs,\n",
        "                                                    learning_rate)                          \n",
        "    elif self.optimizer is 'MBGD':\n",
        "      self.optimized_weights = self.mini_batch_gd(X_train, y_train,\n",
        "                                                  verbose, epochs=epochs,\n",
        "                                                  batch_size = batch_size)                                     \n",
        "    else:\n",
        "      self.optimized_weights = self.stochastic_gd(X_train, y_train,\n",
        "                                                  verbose, epochs=epochs)\n",
        "    self.weights = self.all_weights\n",
        "                                                  \n",
        "  def predict(self, X):\n",
        "    X = self.add_dummy_feature(X)\n",
        "    assert X.shape[-1] == self.optimized_weights.shape[0], 'Incompatible Shapes'\n",
        "    return X @ self.optimized_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples = 10000)\n",
        "y = y.reshape(-1,1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)"
      ],
      "metadata": {
        "id": "R2F20naZ4Vfj"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "e = 10"
      ],
      "metadata": {
        "id": "_4reAHOC9i4D"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = time.time()\n",
        "model_gd = LinearRegression(optimizer = 'GD')\n",
        "model_gd.train(x_train, y_train, epochs = 10, verbose = True, learning_rate = 0.0001)\n",
        "print(time.time() - a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_pPwKWT9YUq",
        "outputId": "e79f3c17-dd94-45ea-fea5-fd99282cbe1e"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Current Loss is : 257986103.63728854\n",
            "The Current Loss is : 13082377.031345716\n",
            "The Current Loss is : 1089717.733753635\n",
            "The Current Loss is : 107598.11621560548\n",
            "The Current Loss is : 11453.56003601751\n",
            "The Current Loss is : 1269.9388190657373\n",
            "The Current Loss is : 144.51802903685052\n",
            "The Current Loss is : 16.75490393676003\n",
            "The Current Loss is : 1.970597825032935\n",
            "The Current Loss is : 0.23448419739197524\n",
            "0.06466317176818848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = time.time()\n",
        "model_mbgd = LinearRegression(optimizer = 'MBGD')\n",
        "model_mbgd.train(x_train, y_train, batch_size = 100, epochs = e, verbose = True)\n",
        "print(time.time() - a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUA7g6n79YII",
        "outputId": "f306fe0d-232b-414c-d9cf-37d13cbd6064"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss is : 1.78015282543965e-05\n",
            "Epoch 1 Loss is : 2.4507867883945274e-18\n",
            "Epoch 2 Loss is : 1.1526821658053038e-23\n",
            "Epoch 3 Loss is : 1.116941108782542e-23\n",
            "Epoch 4 Loss is : 1.0512088676956352e-23\n",
            "Epoch 5 Loss is : 1.0856336235156324e-23\n",
            "Epoch 6 Loss is : 1.069286808433116e-23\n",
            "Epoch 7 Loss is : 1.0599778840174116e-23\n",
            "Epoch 8 Loss is : 1.0752533563490111e-23\n",
            "Epoch 9 Loss is : 1.060598643555592e-23\n",
            "0.09479856491088867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = time.time()\n",
        "model_sgd = LinearRegression(optimizer = 'SGD')\n",
        "model_sgd.train(x_train, y_train, epochs = e, verbose = True)\n",
        "print(time.time() - a)"
      ],
      "metadata": {
        "id": "gyTUYaYlKz2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15fc408-4049-47e7-9284-de2a7ce73ecd"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss is : 0.0015262366480492296\n",
            "Epoch 1 Loss is : 8.688210971476476e-14\n",
            "Epoch 2 Loss is : 1.705728411008878e-21\n",
            "Epoch 3 Loss is : 1.3138694677457339e-21\n",
            "Epoch 4 Loss is : 1.2844493463757655e-21\n",
            "Epoch 5 Loss is : 1.2564962356337583e-21\n",
            "Epoch 6 Loss is : 1.2444885972753088e-21\n",
            "Epoch 7 Loss is : 1.2193577770824084e-21\n",
            "Epoch 8 Loss is : 1.2267829178621826e-21\n",
            "Epoch 9 Loss is : 1.223819860002979e-21\n",
            "1.3747100830078125\n"
          ]
        }
      ]
    }
  ]
}